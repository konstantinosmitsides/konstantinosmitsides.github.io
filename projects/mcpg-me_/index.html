<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MCPG-ME | Konstantinos Mitsides </title> <meta name="author" content="Konstantinos Mitsides"> <meta name="description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="Konstantinos Mitsides"> <meta property="og:type" content="website"> <meta property="og:title" content="Konstantinos Mitsides | MCPG-ME"> <meta property="og:url" content="https://konstantinosmitsides.github.io/projects/mcpg-me_/"> <meta property="og:description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="MCPG-ME"> <meta name="twitter:description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta name="twitter:site" content="@k_mitsides"> <meta name="twitter:creator" content="@k_mitsides"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Konstantinos Mitsides"
        },
        "url": "https://konstantinosmitsides.github.io/projects/mcpg-me_/",
        "@type": "WebSite",
        "description": "Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.",
        "headline": "MCPG-ME",
        
        "sameAs": ["https://github.com/konstantinosmitsides", "https://www.linkedin.com/in/konstantinos-mitsides-2029891ba", "https://twitter.com/k_mitsides", "https://medium.com/@k.mitsides"],
        
        "name": "Konstantinos Mitsides",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo_recent.png?1526cffa7efeb2614751611647ba0047"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://konstantinosmitsides.github.io/projects/mcpg-me_/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <title>MCPG-ME | blank</title> <meta name="generator" content="Jekyll v4.3.4"> <meta property="og:title" content="MCPG-ME"> <meta property="og:locale" content="en"> <meta name="description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta property="og:description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <link rel="canonical" href="https://konstantinosmitsides.github.io/projects/mcpg-me_/"> <meta property="og:url" content="https://konstantinosmitsides.github.io/projects/mcpg-me_/"> <meta property="og:site_name" content="blank"> <meta property="og:type" content="article"> <meta property="article:published_time" content="2024-12-17T02:15:46+00:00"> <meta name="twitter:card" content="summary"> <meta property="twitter:title" content="MCPG-ME"> <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-12-17T02:15:46+00:00","datePublished":"2024-12-17T02:15:46+00:00","description":"Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.","headline":"MCPG-ME","mainEntityOfPage":{"@type":"WebPage","@id":"https://konstantinosmitsides.github.io/projects/mcpg-me_/"},"url":"https://konstantinosmitsides.github.io/projects/mcpg-me_/"}</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Konstantinos</span> Mitsides </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MCPG-ME</h1> <p class="post-description">Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high-level_illustration.png" sizes="95vw"></source> <img src="/assets/img/high-level_illustration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="MCPG-ME" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The MCPG-ME method employs two distinct variation operators within the standard MAP-Elites loop: (1) \(\textbf{Isoline Variation}\), which mutates a parent solution based on the genotype of a randomly selected elite; (2) \(\textbf{MCPG Variation}\), in which a parent and a random offspring from previous iterations are transformed to the episodic action vector using \(\boldsymbol{F}\) for enriched representation, adjusted with \(\boldsymbol{P}\) based on quality metrics, and then reconverted to the genotype space through \(\boldsymbol{J}\), the Jacobian of \(\boldsymbol{F}\). </div> <h1 id="introduction">Introduction</h1> <p>Quality-Diversity (QD) optimisation, a family of evolutionary algorithms designed to produce a <strong>diverse</strong> set of <strong>high-performing</strong> solutions for a specific problem. MAP-Elites <a class="citation" href="#paper14">(Mouret &amp; Clune, 2015)</a> is a simple, yet effective QD optimisation algorithm that has shown success to a variety of fields. It has been used to teach robots how to adapt to damage <a class="citation" href="#paper9">(Cully et al., 2015; Vassiliades et al., 2016; Tarapore et al., 2016)</a>, generate aerodynamic designs <a class="citation" href="#paper46">(Gaier et al., 2017)</a> or to create content for games <a class="citation" href="#paper47">(Alvarez et al., 2017)</a>. MAP-Elites, however, is driven by Genetic Algorithms and often struggles to evolve neural networks with numerous parameters and is <strong>inefficient in navigating the search space</strong> of the optimisation problem. Additionally, actor-critic based QD algorithms, like PGA-MAP-Elites and DCRL, while capable of handling more complex models efficiently, suffer from <strong>slow execution times</strong> and are heavily dependent on the effectiveness of the actor-critic training, which <strong>compromises scalability</strong>. Addressing these challenges, this work introduces the Monte Carlo Policy Gradient MAP-Elites (MCPG-ME) algorithm, which utilises a Monte Carlo Policy Gradient (MCPG) based variation operator to apply quality-guided mutations. This novel variation operator allows MCPG-ME to independently optimise the solutions without relying on any actor-critic training, enhancing <strong>scalability</strong>, <strong>runtime efficiency</strong> while maintaining <strong>competitive sample efficiency</strong>.</p> <h3 id="results">Results</h3> <p>Evaluations across various continuous control locomotion tasks demonstrate that MCPG-ME:</p> <ul> <li> <p><strong>Fast:</strong> Achieves high execution speeds, operating significantly faster than the actor-critic based QD algorithms, in some cases running up to nine times faster.</p> </li> <li> <p><strong>Sample Efficient:</strong> Surpasses the performance of the state-of-the-art algorithm, DCRL, in some of the tasks and consistently outperform all the other baselines in most of the tasks.</p> </li> <li> <p><strong>Scalable:</strong> Demonstrates promising scalability capabilities when subjected to massive parallelisation.</p> </li> <li> <p><strong>Novel Solutions:</strong> Excels in finding a diverse set of solutions, achieving equal or higher diversity score (coverage) than all competitors in all tasks.</p> </li> </ul> <h1 id="background">Background</h1> <h2 id="map-elites">MAP-Elites</h2> <p>MAP-Elites <a class="citation" href="#paper14">(Mouret &amp; Clune, 2015)</a> is a simple, yet effective QD optimisation algorithm that has shown success to a variety of fields. It has been used to teach robots how to adapt to damage <a class="citation" href="#paper9">(Cully et al., 2015; Vassiliades et al., 2016; Tarapore et al., 2016)</a>, generate aerodynamic designs <a class="citation" href="#paper46">(Gaier et al., 2017)</a> or to create content for games <a class="citation" href="#paper47">(Alvarez et al., 2017)</a>. Its objective is to find a diverse set of high-performing solutions to a specific task, like any QD algorithm.</p> <p>First, a user chooses a fitness function \(f(x)\) that evaluates a solution \(x\). For instance, if searching for robot locomotions, the fitness function could be how fast the robot could move from one specified location to another. Second, the user chooses \(N\) dimensions of variation of interest that define a feature space of interest to the user—the Behavioural Descriptor (BD) space. For robot locomotions, one dimension of interests could be the time duration a specific foot is in touch with the ground in one episode. Similarly, other dimensions could represent the time duration corresponding to the other feet of the robot. Each dimension of variation is discretised based on user preference or available computational resources, and given a particular discretisation, MAP-Elites will search for the fittest solution for each cell in the \(N\)-dimensional BD space. This search is conducted in the space of all possible values of \(x\)—the search space—where \(x\) is the description vector of a candidate solution. In the EAs vocabulary, the descriptor \(x\) is called a genotype, and the robot locomotion, in our example, is the phenotype, \(p_{x}\). Moreover, a behaviour function, \(b(x)\) must exist in order to map the genotype to an \(N\)-dimensional vector, \(\mathbf{b}_{\mathbf{x}}\), which describes \(x\)’s features.</p> <p>MAP-Elites begins by randomly generating \(K \in \mathbb{N}\) genotypes and assessing each for fitness (performance) and features (behaviour). These genotypes are then assigned to corresponding cells in the BD space. If multiple genotypes map to the same cell, only the fittest one is retained. Once initialised, the algorithm proceeds with the following steps, repeated until a termination criterion is met (see Figure 1):</p> <p><i>i.</i> <strong>Selection Mechanism</strong>: A genotype is randomly selected from one of the cells in the map.</p> <p><i>ii.</i> <strong>Genetic Algorithms (GA) variation</strong>: The selected genotype produces an offspring through mutation—introducing random changes to the genotype (injecting noise into the parameters)—and/or crossover, which combines the genotypes of two parents.</p> <p><i>iii.</i> <strong>Evaluation</strong>: The offspring’s behaviour and fitness are evaluated.</p> <p><i>iv.</i> <strong>Addition Mechanism</strong>: The offspring is assigned to a cell based on its behavioral characteristics. If the cell is empty, the offspring is placed directly. If the cell is occupied, the offspring replaces the current occupant only if it has higher fitness; otherwise, the original occupant is retained and the offspring is discarded, or ‘<em>dies</em>’.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ME.png" sizes="95vw"></source> <img src="/assets/img/ME.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Main cycle of MAP-ELites. Adapted from <a class="citation" href="#MAP_Eites_vis">(J.-B Mouret, 2015)</a>. This figure represents a task with a 2-dimensional BD space. </div> <h2 id="synergy-between-map-elites-and-deep-reinforcement-learning">Synergy between MAP-Elites and Deep Reinforcement Learning</h2> <p>To overcome the limitations of GAs in MAP-Elites, specifically their inefficiency and inadequacy for evolving high-dimensional neural networks, significant advancements have been made by integrating Deep Reinforcement Learning (DRL) <a class="citation" href="#paper20">(Mnih et al., 2016; Mnih et al., 2013)</a> into the MAP-Elites framework. Specifically, traditional GA-based variation operators are enhanced with Policy Gradient (PG) <a class="citation" href="#paper22">(Williams, 1992; Sutton et al., 2018)</a> methods. These methods, a specialised branch of DRL, are particularly effective for training large neural networks to navigate high-dimensional state spaces and manage continuous action spaces <a class="citation" href="#paper24">(Haarnoja et al., 2018; Lillicrap et al., 2015; Silver et al., 2019)</a>. Their integration into MAP-Elites leverages these capabilities, rendering the combined framework more capable of tackling complex optimisation challenges.</p> <p>All the PG-based MAP-Elites algorithms discussed in this work follow the conventional MAP-Elites algortihm, where in addition to the GA-based variation operator they have a PG-based variation operator, as seen in Figure 2. Thus, in each iteration, half of the solutions are being optimised by the PG operator and the rest by the GA operator.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/PG-ME.png" sizes="95vw"></source> <img src="/assets/img/PG-ME.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: The PG &amp; MAP-Elites pipeline processes half of the solutions using the GA emitter, where they undergo random genetic variation, as outlined in step ii of Figure 1. The remaining solutions are processed by the PG emitter, where they are updated using PG methods. </div> <p>Currently, the state-of-the-art QD-PG algorithms are the actor-critic based MAP-Elites algorithms, namely PGA-MAP-Elites <a class="citation" href="#paper27">(Nilsson &amp; Cully, 2021)</a> and DCG-MAP-Elites-AI <a class="citation" href="#paper28">(<i>Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning</i>, 2023; Faldor et al., 2023)</a>. In PGA-MAP-Elites, actor-critic training similar to the TD3 <a class="citation" href="#TD3">(Fujimoto et al., 2018)</a> algorithm takes place <strong>globally</strong> and is <strong>independent</strong> of the solution optimisation occurring in the PG emitter. In each iteration, the solutions processed by the PG emitter use the trained critic from the actor-critic training to obtain more informed gradient estimates, leading to more efficient optimisation of solutions. To address the limitations of PGA-MAP-Elites, especially its shortcomings on tasks where the fitness objective directly discourages diversity in solutions, DCG-MAP-Elites-AI conditions the actor-critic training on the descriptors of the solutions. The high-level pipeline of the PG emitter in these two actor-critic based MAP-Elites algorithms is depicted in Figure 3.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/actorcritic_training.jpeg" sizes="95vw"></source> <img src="/assets/img/actorcritic_training.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: Pipeline of the PG emitter in the actor-critic based MAP-Elites algorithms. </div> <h1 id="problem">Problem</h1> <p>While the state-of-the-art QD-PG (actor-critic MAP-Elites) algorithms excel in sample efficiency, they have:</p> <ul> <li> <p><strong>High training time:</strong> Substantial time required for actor-critic training.</p> </li> <li> <p><strong>Poor scalability:</strong> Actor-critic training requires a sufficient number of sequential learning steps to converge and the trained critic is shared across all solutions. For further explanation about this cause, please refer to the ‘Scalability’ subsection in the ‘Main Results’ of the ‘Experiments’ chapter in the <a href="/assets/pdf/thesis_report_msc.pdf">report</a>.</p> </li> </ul> <h1 id="goal">Goal</h1> <p>The goal of this project is to synergise PG methods with MAP-Elites to develop an algorithm that combines the fast execution and scalability of MAP-Elites with the efficient optimisation capabilities of PG methods. Specifically, we aim to achieve performance comparable to actor-critic based MAP-Elites algorithms but with significantly reduced runtime. Furthermore, we intend for the algorithm to remain scalable, meaning that additional hardware resources can enhance final performance or reduce training time without compromising final performance.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Goal.jpeg" sizes="95vw"></source> <img src="/assets/img/Goal.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: Project Goal. </div> <h1 id="methods">Methods</h1> <p>To achieve this goal, the Monte Carlo Policy Gradient MAP-Elites (MCPG-ME) algorithm is developed. This algorithm removes the global actor-critic training seen in the state-of-the-art algorithms, as depicted in Figure 3, and constructs an objective function. This function allows the Monte Carlo Policy Gradient (MCPG) methods to operate effectively in an off-policy manner, enabling the acquisition of useful gradient estimates with minimal data. Particularly, as shown in Figure 5, each solution processed by the PG emitter in each iteration undergoes the following steps:</p> <p><i>i.</i> <strong>Trajectory Sampling:</strong> Randomly sample \(N\) trajectories from the offspring evaluations of the previous generation.</p> <p><i>ii.</i> <strong>Objective Function Computation:</strong> States, actions and adjusted rewards-to-go are used to compute the objective function.</p> <p><i>iii.</i> <strong>Policy Update:</strong> Parameters are updated \(n\) times in the direction of the gradient.</p> <p><i>iv.</i> <strong>Evaluation &amp; Trajectory Buffer Update:</strong> Offspring evaluated for archive inclusion; buffer refreshed with current evaluation trajectories.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/solution.png" sizes="95vw"></source> <img src="/assets/img/solution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5: Pipeline of the PG emitter in MCPG-ME algorithm. </div> <p>For developement and technical details regarding MCPG-ME please refer to the ‘Methodology’ chapter of the <a href="/assets/pdf/thesis_report_msc.pdf">report</a>. The results of MCPG-ME are outlined in the ‘Introduction’ at the top of this page. For a more comprehensive understanding of the experiments and the implications of the experimental results, please refer to the ‘Experiments’ chapter of the <a href="/assets/pdf/thesis_report_msc.pdf">report</a>.</p> <h2 id="author">Author</h2> <ul> <li>Konstantinos Mitsides</li> </ul> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper28" class="col-sm-8"> <div class="title">Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning</div> <div class="author"> </div> <div class="periodical"> May 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper29" class="col-sm-8"> <div class="title">MAP-Elites with Descriptor-Conditioned Gradients and Archive Distillation into a Single Policy</div> <div class="author"> Maxence Faldor, Félix Chalumeau, Manon Flageat, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Antoine Cully' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> May 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper27" class="col-sm-8"> <div class="title">Policy Gradient Assisted MAP-Elites; Policy Gradient Assisted MAP-Elites</div> <div class="author"> Olle Nilsson, and Antoine Cully </div> <div class="periodical"> <em></em> May 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper26" class="col-sm-8"> <div class="title">Deterministic Policy Gradient Algorithms</div> <div class="author"> David Silver, Nicolas Heess, Thomas Degris, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Daan Wierstra, Martin Riedmiller' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> May 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper23" class="col-sm-8"> <div class="title">Policy Gradient Methods for Reinforcement Learning with Function Approximation</div> <div class="author"> Richard S Sutton, David Mcallester, Satinder Singh, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Yishay Mansour' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> May 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper24" class="col-sm-8"> <div class="title">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</div> <div class="author"> Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sergey Levine' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TD3" class="col-sm-8"> <div class="title">Addressing Function Approximation Error in Actor-Critic Methods</div> <div class="author"> Scott Fujimoto, Herke Hoof, and David Meger </div> <div class="periodical"> <em>CoRR</em>, Jan 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper46" class="col-sm-8"> <div class="title">Aerodynamic design exploration through surrogate-assisted illumination</div> <div class="author"> Adam Gaier, Alexander Asteroth, and Jean Baptiste Mouretz </div> <div class="periodical"> <em>In 18th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference, 2017</em>, Jan 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper47" class="col-sm-8"> <div class="title">IEEE TRANSACTION ON GAMES, VOL. XX, NO. X, MONTH XXXX 1 Interactive Constrained MAP-Elites: Analysis and Evaluation of the Expressiveness of the Feature Dimensions</div> <div class="author"> Alberto Alvarez, Steve Dahlskog, Jose Font, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Julian Togelius' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Jan 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper10" class="col-sm-8"> <div class="title">Using Centroidal Voronoi Tessellations to Scale Up the Multi-dimensional Archive of Phenotypic Elites Algorithm</div> <div class="author"> Vassilis Vassiliades, Konstantinos Chatzilygeroudis, and Jean-Baptiste Mouret </div> <div class="periodical"> <em></em> Oct 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper11" class="col-sm-8"> <div class="title">How Do Different Encodings Influence the Performance of the MAP-Elites Algorithm?</div> <div class="author"> Danesh Tarapore, Jeff Clune, Antoine Cully, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jean-Baptiste Mouret' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> Oct 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper20" class="col-sm-8"> <div class="title">Asynchronous Methods for Deep Reinforcement Learning</div> <div class="author"> Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Alex Graves, Tim Harley, Timothy P Lillicrap, David Silver, Koray Kavukcuoglu' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> Oct 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper14" class="col-sm-8"> <div class="title">Illuminating search spaces by mapping elites</div> <div class="author"> Jean-Baptiste Mouret, and Jeff Clune </div> <div class="periodical"> <em></em> Apr 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper9" class="col-sm-8"> <div class="title">Robots that can adapt like animals</div> <div class="author"> Antoine Cully, Jeff Clune, Danesh Tarapore, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jean Baptiste Mouret' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Nature</em>, May 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="MAP_Eites_vis" class="col-sm-8"> <div class="title">Illuminating search spaces by mapping elites.</div> <div class="author"> J. Clune J.-B Mouret </div> <div class="periodical"> May 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper25" class="col-sm-8"> <div class="title">Continuous control with deep reinforcement learning</div> <div class="author"> Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em></em> Sep 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper21" class="col-sm-8"> <div class="title">Playing Atari with Deep Reinforcement Learning</div> <div class="author"> Volodymyr Mnih, Koray Kavukcuoglu, David Silver, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em></em> Dec 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">1992</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="paper22" class="col-sm-8"> <div class="title">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</div> <div class="author"> Ronald J Williams </div> <div class="periodical"> Dec 1992 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Konstantinos Mitsides. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"AI Research & Software Engineering Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-gnbyn",title:"GNByN",description:"GraphNETbyNET: A novel architecture of Graph Neural Netowrks (GNNs), inspired by a variety of GNN and NN architectures to generate high-resolution brain connectivity graphs from low-resolution ones.",section:"Projects",handler:()=>{window.location.href="/projects/GNByN/"}},{id:"projects-kiraka",title:"Kiraka",description:"Speed Reading Platform",section:"Projects",handler:()=>{window.location.href="/projects/kiraka/"}},{id:"projects-mcpg-me",title:"MCPG-ME",description:"Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.",section:"Projects",handler:()=>{window.location.href="/projects/mcpg-me/"}},{id:"projects-mcpg-me",title:"MCPG-ME",description:"Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.",section:"Projects",handler:()=>{window.location.href="/projects/mcpg-me_/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B.%6D%69%74%73%69%64%65%73@%63%79%74%61%6E%65%74.%63%6F%6D.%63%79","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/konstantinosmitsides","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/konstantinos-mitsides-2029891ba","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/k_mitsides","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@k.mitsides","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>