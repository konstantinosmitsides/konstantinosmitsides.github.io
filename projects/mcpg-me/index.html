<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MCPG-ME | Konstantinos Mitsides </title> <meta name="author" content="Konstantinos Mitsides"> <meta name="description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="Konstantinos Mitsides"> <meta property="og:type" content="website"> <meta property="og:title" content="Konstantinos Mitsides | MCPG-ME"> <meta property="og:url" content="https://konstantinosmitsides.github.io/projects/mcpg-me/"> <meta property="og:description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="MCPG-ME"> <meta name="twitter:description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta name="twitter:site" content="@k_mitsides"> <meta name="twitter:creator" content="@k_mitsides"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Konstantinos Mitsides"
        },
        "url": "https://konstantinosmitsides.github.io/projects/mcpg-me/",
        "@type": "WebSite",
        "description": "Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.",
        "headline": "MCPG-ME",
        
        "sameAs": ["https://scholar.google.com/citations?user=wGBxqc8AAAAJ&hl=en", "https://github.com/konstantinosmitsides", "https://www.linkedin.com/in/konstantinos-mitsides-2029891ba", "https://twitter.com/k_mitsides"],
        
        "name": "Konstantinos Mitsides",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://konstantinosmitsides.github.io/projects/mcpg-me/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <title>MCPG-ME | blank</title> <meta name="generator" content="Jekyll v4.4.1"> <meta property="og:title" content="MCPG-ME"> <meta property="og:locale" content="en"> <meta name="description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <meta property="og:description" content="Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms."> <link rel="canonical" href="https://konstantinosmitsides.github.io/projects/mcpg-me/"> <meta property="og:url" content="https://konstantinosmitsides.github.io/projects/mcpg-me/"> <meta property="og:site_name" content="blank"> <meta property="og:type" content="article"> <meta property="article:published_time" content="2026-02-11T20:01:07+00:00"> <meta name="twitter:card" content="summary"> <meta property="twitter:title" content="MCPG-ME"> <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2026-02-11T20:01:07+00:00","datePublished":"2026-02-11T20:01:07+00:00","description":"Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.","headline":"MCPG-ME","mainEntityOfPage":{"@type":"WebPage","@id":"https://konstantinosmitsides.github.io/projects/mcpg-me/"},"url":"https://konstantinosmitsides.github.io/projects/mcpg-me/"}</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Konstantinos</span> Mitsides </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MCPG-ME</h1> <p class="post-description">Monte Carlo Policy Gradient MAP-Elites: A synergy between Deep Reinforcement Learning and Quality Diversity Algorithms.</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/high-level_illustration.png" sizes="95vw"></source> <img src="/assets/img/high-level_illustration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="MCPG-ME" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The MCPG-ME method employs two distinct variation operators within the standard MAP-Elites loop: (1) \(\textbf{Isoline Variation}\), which mutates a parent genotype based on that of a randomly selected elite; (2) \(\textbf{MCPG Variation}\), where the behavior of a parent genotype is computed over a trajectory of states previously encountered by another. The behavior is then adjusted with \(\boldsymbol{P}\) based on quality metrics estimated from comparisons with the second genotype's behavior. This adjusted behavior is converted back into genotype space using \(\boldsymbol{J}\) to mutate the parent genotype. </div> <h1 id="summary">Summary</h1> <p>Quality-Diversity (QD) optimization, a family of evolutionary algorithms designed to produce a <strong>diverse</strong> set of <strong>high-performing</strong> solutions for a specific problem. MAP-Elites (missing reference) is a simple, yet effective QD optimization algorithm that has shown success to a variety of fields. It has been used to teach robots how to adapt to damage (missing reference), generate aerodynamic designs (missing reference) or to create content for games (missing reference). MAP-Elites, however, is driven by Genetic Algorithms and often struggles to evolve neural networks with numerous parameters and is <strong>inefficient in navigating the search space</strong> of the optimization problem. Additionally, actor-critic based QD algorithms, like PGA-MAP-Elites (missing reference) and DCRL (missing reference) , while capable of handling more complex models efficiently, suffer from <strong>slow execution times</strong> and are heavily dependent on the effectiveness of the actor-critic training, which <strong>compromises scalability</strong>. Addressing these challenges, this work introduces the Monte Carlo Policy Gradient MAP-Elites (MCPG-ME) algorithm, which utilizes a Monte Carlo Policy Gradient (MCPG) based variation operator to apply quality-guided mutations to the solutions. This novel variation operator allows MCPG-ME to independently optimize the solutions without relying on any actor-critic training, enhancing <strong>scalability</strong>, <strong>runtime efficiency</strong> while maintaining <strong>competitive sample efficiency</strong>.</p> <h3 id="results">Results</h3> <p>Evaluations across various continuous control locomotion tasks demonstrate that MCPG-ME:</p> <ul> <li> <p><strong>Fast:</strong> Achieves high execution speeds, operating significantly faster than the actor-critic based QD algorithms, in some cases running up to nine times faster.</p> </li> <li> <p><strong>Sample Efficient:</strong> Surpasses the performance of the state-of-the-art algorithm, DCRL, in some of the tasks and consistently outperform all the other baselines in most of the tasks.</p> </li> <li> <p><strong>Scalable:</strong> Demonstrates promising scalability capabilities when subjected to massive parallelization.</p> </li> <li> <p><strong>Novel Solutions:</strong> Excels in finding a diverse set of solutions, achieving equal or higher diversity score (coverage) than all competitors in all tasks.</p> </li> </ul> <p>In these tasks, solutions refer to the robot’s walking behavior. The <strong>quality</strong> of solutions refers to the effectiveness with which the robot completes a given task, and the <strong>diversity</strong> of solutions denotes the range of different walking styles the robot can successfully employ. See below some interesting solutions:</p> <div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap;"> <div style="flex: 1 0 33%; text-align: center; padding: 5px;"> <img src="/assets/img/jumping_walker.gif" alt="A jumping Walker" style="max-width: 200px; height: 155px; object-fit: cover;"> <p style="text-align: center;">A hopping Walker</p> </div> <div style="flex: 1 0 33%; text-align: center; padding: 5px;"> <img src="/assets/img/anttrap_omni_vis.gif" alt="Ant going around the trap" style="max-width: 200px; height: 155px; object-fit: cover;"> <p style="text-align: center;">Ant going around the trap</p> </div> <div style="flex: 1 0 33%; text-align: center; padding: 5px;"> <img src="/assets/img/walker_normal.gif" alt="A fast Walker" style="max-width: 200px; height: 155px; object-fit: cover;"> <p style="text-align: center;">A fast Walker</p> </div> </div> <p>For technical details of the method and the official results, please stay tuned. The paper is coming soon!</p> <h2 id="author">Author</h2> <ul> <li> <em>Konstantinos Mitsides</em> <strong>(konstantinos.mistides23@imperial.ac.uk)</strong> </li> </ul> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Konstantinos Mitsides. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>